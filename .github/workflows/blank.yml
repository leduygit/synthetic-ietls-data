name: IELTS Essay Generator Pipeline

on:
  push:
    paths:
      - "readme.md" # Trigger only when README.md changes

jobs:
  generate:
    runs-on: ubuntu-latest
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      KAGGLE_JSON: ${{ secrets.KAGGLE_JSON }}
    steps:
      - uses: actions/checkout@v3

      - name: Verify env
        run: |
          if [ -z "$OPENAI_API_KEY" ]; then
            echo "❌ OPENAI_API_KEY is not set!"
            exit 1
          else
            echo "✅ OPENAI_API_KEY is available"
          fi

  pipeline:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt kaggle

      - name: Update CA certificates
        run: sudo apt-get update && sudo apt-get install -y ca-certificates

      - name: Debug OpenAI API with curl
        run: |
          curl -v https://api.openai.com/v1/models \
            -H "Authorization: Bearer $OPENAI_API_KEY"
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

      - name: Check README value
        id: readme
        run: |
          value=$(tail -n 1 readme.md)
          echo "lastval=$value" >> $GITHUB_ENV
          if [ "$value" -eq 5 ]; then
            echo "Value is 5, stopping workflow."
            exit 0
          fi

      - name: Run with IPv4 only
        run: |
          python - <<'EOF'
          import os, socket
          import requests.packages.urllib3.util.connection as urllib3_cn

          def allowed_gai_family():
              return socket.AF_INET
          urllib3_cn.allowed_gai_family = allowed_gai_family

          from openai import OpenAI
          client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

          try:
              resp = client.chat.completions.create(
                  model="gpt-4o-mini",
                  messages=[{"role": "user", "content": "Hello from IPv4-only run"}],
                  max_tokens=10
              )
              print("✅ Success:", resp.choices[0].message.content)
          except Exception as e:
              print("❌ Failed:", e)
          EOF
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

      - name: Parse parameters from commit message
        id: parse_params
        run: |
          COMMIT_MESSAGE=$(git log -1 --pretty=%B)
          echo "Commit message: $COMMIT_MESSAGE"
      
          BAND_SCORE=$(echo "$COMMIT_MESSAGE" | grep -oP 'band_score=\K\d+')
          BATCH_SIZE=$(echo "$COMMIT_MESSAGE" | grep -oP 'batch_size=\K\d+')
          NUM_ESSAYS=$(echo "$COMMIT_MESSAGE" | grep -oP 'num_essays=\K\d+')
          
          echo "BAND_SCORE=$BAND_SCORE" >> $GITHUB_ENV
          echo "BATCH_SIZE=$BATCH_SIZE" >> $GITHUB_ENV
          echo "NUM_ESSAYS=$NUM_ESSAYS" >> $GITHUB_ENV


      - name: Run essay generator
        run: |
          python gen-data.py $BAND_SCORE --num-essays $NUM_ESSAYS
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}


      # 🔽 Configure Kaggle API
      - name: Configure Kaggle API
        run: |
          mkdir -p ~/.kaggle
          echo "{\"username\":\"$KAGGLE_USERNAME\",\"key\":\"$KAGGLE_KEY\"}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}

       # 🔽 Pull dataset metadata + files
      - name: Pull Kaggle dataset (metadata + files)
        run: |
          kaggle datasets download -d thanhnghia123/mielband-ielts-data-train -p ./dataset_meta --unzip
          kaggle datasets metadata thanhnghia123/mielband-ielts-data-train -p ./dataset_meta

      # 🔽 Patch dataset metadata with correct ID
      - name: Patch dataset-metadata.json with ID
        run: |
          cat ./dataset_meta/dataset-metadata.json | jq -r '.' | jq '.id = "thanhnghia123/mielband-ielts-data-train"' > ./dataset_meta/tmp.json
          mv ./dataset_meta/tmp.json ./dataset_meta/dataset-metadata.json
          echo "✅ Patched dataset-metadata.json with ID"

      # 🔽 Add generated CSV into existing training/ folder
      - name: Add new CSV into dataset folder
        run: |
          latest_csv=$(ls -t band_*.csv | head -n 1)
          echo "📂 Found CSV: $latest_csv"

          mkdir -p ./dataset_meta/training
          cp "$latest_csv" ./dataset_meta/training/

      # 🔽 Push updated dataset with all files (old + new)
      - name: Push dataset to Kaggle
        run: |
          kaggle datasets version -p ./dataset_meta -m "Update dataset with new file" -r zip


      # 🔽 Pull notebook code + metadata into one folder
      - name: Pull Kaggle notebook (code + metadata)
        run: kaggle kernels pull blueinthe/train -p ./notebook_meta -m

      # 🔽 Ensure kernel-metadata.json has correct ID and code_file
      - name: Patch kernel-metadata.json with ID and notebook file
        run: |
          jq '.id = "blueinthe/train" | .code_file = "train.ipynb"' ./notebook_meta/kernel-metadata.json > ./notebook_meta/tmp.json
          mv ./notebook_meta/tmp.json ./notebook_meta/kernel-metadata.json
          echo "✅ Patched kernel-metadata.json with correct ID and code_file"

      # 🔽 Push Kaggle Notebook
      - name: Push Kaggle Notebook
        run: kaggle kernels push -p ./notebook_meta








